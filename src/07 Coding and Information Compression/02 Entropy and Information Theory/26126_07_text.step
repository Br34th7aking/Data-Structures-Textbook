{"block": {"name": "text", "text": "<p>In the previous example, the string <b><span class=\"wysiwyg-color-green\">A</span><span class=\"wysiwyg-color-blue\">C</span><span class=\"wysiwyg-color-purple\">G</span><span class=\"wysiwyg-color-red\">T</span></b> was represented by a single byte (<b><span class=\"wysiwyg-color-green\">00</span><span class=\"wysiwyg-color-blue\">01</span><span class=\"wysiwyg-color-purple\">10</span><span class=\"wysiwyg-color-red\">11</span></b>). However, what about the string <b><span class=\"wysiwyg-color-green\">AAAAA<span class=\"wysiwyg-color-blue\">C<span class=\"wysiwyg-color-purple\">G</span></span><span class=\"wysiwyg-color-red\">T</span></span></b>? Clearly we can represent it in two bytes (<b><span class=\"wysiwyg-color-green\">00000000 00<span class=\"wysiwyg-color-blue\">01<span class=\"wysiwyg-color-purple\">10</span></span><span class=\"wysiwyg-color-red\">11</span></span></b>), but now that we've seen the message, can we use the extra knowledge we've gained about the message in order to better encode it? Since <b><span class=\"wysiwyg-color-green\">A</span></b> is the most frequent letter in our string, we can be clever and represent it using less than 2 bits, and in exchange, we can represent some other (less frequent) character using more than 2 bits. Although we will have a loss as far as compression goes when we encounter the longer-represented less-frequent character, we will have a gain when we encounter the shorter-represented more-frequent character. Let's map <b><span class=\"wysiwyg-color-green\">A\u00a0\u2192 0</span></b>, <b><span class=\"wysiwyg-color-blue\">C\u00a0\u2192 10</span></b>, <b><span class=\"wysiwyg-color-purple\">G\u00a0\u2192 110</span></b>, and <b><span class=\"wysiwyg-color-red\">T\u00a0\u2192 111</span></b>:</p><p class=\"wysiwyg-text-align-center\"><b><span class=\"wysiwyg-color-green\">AAAAA</span><span class=\"wysiwyg-color-blue\">C</span><span class=\"wysiwyg-color-purple\">G</span><span class=\"wysiwyg-color-red\">T</span>\u00a0\u2192 <span class=\"wysiwyg-color-green\">00000</span><span class=\"wysiwyg-color-blue\">10</span><span class=\"wysiwyg-color-purple\">1 10</span><span class=\"wysiwyg-color-red\">111 <span class=\"wysiwyg-color-black\">(1.625 bytes)</span></span></b></p><p class=\"wysiwyg-text-align-left\">Of course, in memory, we cannot store fractions of bytes, so the number \"1.625 bytes\" is meaningless on its own, but what if we repeated the sequence\u00a0<span class=\"wysiwyg-color-green\"><b>AAAAA</b></span><span class=\"wysiwyg-color-blue\"><b>C</b></span><span class=\"wysiwyg-color-purple\"><b>G</b></span><span class=\"wysiwyg-color-red\"><b>T</b></span>\u00a01,000 times? With the previous naive approach, the resulting file would be 2,000 bytes, but with this approach, the resulting file would be 1,625 bytes, and both of these files are coming from a file that was originally 8,000 bytes (8 ASCII characters repeated 1,000 times). Note that, if we encode a message based on its character frequencies, we need to provide that vital character-frequency information to the recipient of our message so that the recipient can decode our message. To do so, in addition to the 1,625 bytes our compressed message occupies, we need to add enough information for the recipient to reconstruct the coding scheme to the beginning of our file, which causes slight overhead that results in a compressed file of just over 1,625 bytes. we will expand on this \"overhead of added frequency information\" in the upcoming steps.</p><p class=\"wysiwyg-text-align-left\">Basically, in addition to simply limiting our alphabet based on what characters we expect to see (which would get us down from 8,000 bytes to 2,000 bytes in this example), we can further improve our encoding by taking into account the frequencies at which each letter appears (which would get us down even lower than 2,000 bytes in this example). Unfortunately, not everybody cares about Biology, so these results will seem largely uninteresting if our compression technique only supports DNA sequences. Can we take this approach of looking at character frequencies and generalize it?</p>", "video": null, "animation": null, "options": {}, "subtitle_files": [], "source": null, "subtitles": {}, "tests_archive": null, "feedback_correct": "", "feedback_wrong": ""}, "id": "96258", "time": "2017-09-11T11:51:11.089539"}