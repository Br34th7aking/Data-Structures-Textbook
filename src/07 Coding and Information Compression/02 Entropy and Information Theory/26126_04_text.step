{"id": "96255", "block": {"feedback_correct": "", "name": "text", "feedback_wrong": "", "text": "<p>Now that we have formal definitions of the terms <i>data</i> and <i>information</i>, we can begin to tie in\u00a0<b><span class=\"wysiwyg-color-red\">entropy</span></b>, which is in general a measure of the disorder (i.e., non-uniformity) of a system. In the Physical Sciences, entropy measures disorder among molecules, but in Information Theory, entropy (<b><span class=\"wysiwyg-color-red\">Shannon entropy</span></b> in this context) measures the\u00a0unpredictability of information content. Specifically, <i>Shannon entropy</i> is the <b>expected value</b> (i.e., average) of the information contained in some data.</p><p>Say we flip a fair coin. The result of this flip contains 1 bit of information: either the coin landed on heads (1), or it landed on tails (0). However, if we were to flip a biased coin, where both sides are heads, the result of this flip contains 0 bits of information: we know before we even flip the coin that it\u00a0<i>must</i> land on heads, so the actual outcome of the flip doesn't tell us anything we didn't already know. In general, if there are <i>n</i>\u00a0possible outcomes of an event, that event has a value of \u2308log\u2082(<i>n</i>)\u2309 bits of information, and it thus takes\u00a0\u2308log\u2082(<i>n</i>)\u2309 bits of memory to represent the outcomes of this event (which is why it takes \n\n\u2308log\u2082(<i>n</i>)\u2309 bits\u00a0to represent the numbers from 0 to <i>n</i>-1 in binary).</p><p>Notice that we used the term \"uniform\". For our purposes, <b>uniformity</b> refers to the variation (or lack-thereof, specifically) among the symbols in our message. For example, the sequence <b><span class=\"wysiwyg-color-green\">AAAA</span></b> is <b>uniform</b> because only a single symbol appears (<b><span class=\"wysiwyg-color-green\">A</span></b>). The sequence <b><span class=\"wysiwyg-color-green\">A</span><span class=\"wysiwyg-color-blue\">C</span><span class=\"wysiwyg-color-purple\">G</span><span class=\"wysiwyg-color-red\">T</span></b>, however, is <b>non-uniform</b> (assuming an alphabet of only <b>{<span class=\"wysiwyg-color-green\">A</span>, <span class=\"wysiwyg-color-blue\">C</span>, <span class=\"wysiwyg-color-purple\">G</span>, <span class=\"wysiwyg-color-red\">T</span>}</b>), because there is no symbol in our alphabet that appears more frequently than the others. In general, the <i>more unique symbols</i> appear in our message, and the <i>more balanced the frequencies</i> of each unique symbol, the\u00a0<i>higher</i>\ufeff the entropy.</p><p>The topic of entropy gets pretty deep, but for the purposes of this text, the main take away is the following:</p><p class=\"wysiwyg-text-align-center\"><b><span class=\"wysiwyg-color-green\">more uniform data</span>\u00a0\u2192 <span class=\"wysiwyg-color-red\">less entropy</span>\u00a0\u2192 <span class=\"wysiwyg-color-blue\">less information stored in the data</span></b></p><p class=\"wysiwyg-text-align-left\">This simple relationship is the essence of data compression. Basically, if there are <i>k</i> bits of information in a message (for our purposes, let's think of a message as a binary string), but we are representing the message using <i>n</i> bits of memory (where <i>n</i> &gt; <i>k</i>), there is clearly some room for improvement. This is the driving force behind data compression: can we think of a more clever way to encode our message so that the number of bits of memory used to represent it is equal to (or is at least closer to) the number of bits of information the message contains?</p>", "subtitle_files": [], "source": null, "animation": null, "video": null, "tests_archive": null, "options": {}, "subtitles": {}}, "time": "2017-01-05T20:14:21.977733"}