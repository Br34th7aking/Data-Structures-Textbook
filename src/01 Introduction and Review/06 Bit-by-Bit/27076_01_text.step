{"id": "102122", "block": {"feedback_correct": "", "name": "text", "feedback_wrong": "", "text": "<p>In the digital age, people are typically comfortable with the notion of \"bits\" as being \"1s and 0s\" as well as with the notion of \"bytes\" being the fundamental unit of memory on a computer, but the connection between the two is not always obvious.</p><p>A <b><span class=\"wysiwyg-color-green\">bit</span></b> is the basic unit of information in computing and can\u00a0have only one of two values. We typically represent these two values as either a 0 or a 1, but they can be interpreted as logical values (true/false, yes/no), algebraic signs (+/\u2212),\u00a0activation states (on/off), or any other two-valued attribute.<br></p><p>A\u00a0<b><span class=\"wysiwyg-color-blue\">byte</span></b> is a\u00a0unit of digital information, and it is just a sequence of some number of bits. The\u00a0size of the byte has historically been hardware dependent and no definitive standards existed that mandated the size, but for the purposes of this course as well as almost all computer applications, you can assume that a byte is specifically a sequence of <b><span class=\"wysiwyg-color-green\">8 bits</span></b>. Note that, with modern computers, a byte is the smallest unit that can be stored. In other words, a file can be 1 byte, 2 bytes, 3 bytes, etc., but a file\u00a0<i>cannot</i> be 1.5 bytes.</p>", "subtitle_files": [], "source": null, "animation": null, "video": null, "tests_archive": null, "options": {}, "subtitles": {}}, "time": "2017-06-08T22:30:50.448845"}