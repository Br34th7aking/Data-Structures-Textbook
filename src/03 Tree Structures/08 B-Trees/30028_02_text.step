{"block": {"name": "text", "text": "<p>As it turns out, the CPU determines which data to put in which sections of memory largely based on <i>spatial</i> locality. In other words, if we are accessing data that is close to other data, the CPU will also load the close data into an L1 cache because it predicts that we might need to access the close data in the near future. For example, take a look at the for-loop below:</p><p></p><pre><code class=\"cpp\">for(int i = 0; i &lt; 10; i++) {\n\n    a[i] = 0; //Along with a[0] initially, CPU will load a[1], a[2]... into close memory in advance\n\n}</code></pre><p></p><p>The CPU is going to try to load the entire array <span class=\"wysiwyg-font-tt\">a</span> into close memory (the L1 cache) because it predicts that we will probably need to access other sections of the array soon after (and what a coincidence, it's right)! <br></p><p>So, by knowing how the CPU determines which data is placed where in memory, what can this further tell us about accessing memory in tree structures? Well, it tells us that, if a node holds multiple pieces of data (e.g. an employee object that contains <span class=\"wysiwyg-font-tt\">string name</span>, <span class=\"wysiwyg-font-tt\">int age</span>, etc.), then by loading one piece of data from the node (e.g. the employee's name), the CPU will automatically load the rest of the data (e.g. the employee's age, etc.) from the same node. This implies that accessing other data that is <i>inside</i> the node structure is expected to be faster than simply traversing to another node.</p>", "video": null, "animation": null, "options": {}, "subtitle_files": [], "source": null, "subtitles": {}, "tests_archive": null, "feedback_correct": "", "feedback_wrong": ""}, "id": "122783", "time": "2017-09-11T11:47:23.573695"}