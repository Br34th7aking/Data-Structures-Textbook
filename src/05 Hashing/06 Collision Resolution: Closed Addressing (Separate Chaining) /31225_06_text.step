{"block": {"name": "text", "text": "<p>We briefly mentioned in the previous step that, by not checking for duplicates in the<b> insert</b> method, we could get a worst-case <i>constant</i> time complexity. So how does checking for duplicates even work in the first place? We can check for duplicate insertions using two different methods:<br></p><ol><li>Check for duplicates during the <b>insert</b> operation:<ul><li>If we wanted to check for duplicates as we insert a key, we would calculate the index pertaining to the inserting key\u2014using the key's hash function\u2014and then <i>linearly scan</i> the <i>entire</i> linked list at the key's index to check if the same key is present. If the key is present, then we would abort the insert. Otherwise, we would proceed to insert the key. By checking for duplicates in the insert method, we would face a guaranteed worst-case <i>linear</i> time complexity as a result of the linear scan we are forced to do.<br></li></ul></li><li>Check for duplicates during the <b>delete</b> operation:<ul><li>If we wanted to check for duplicates inside the<b> delete </b>operation, we would not have to check for duplicates inside the insert operation (i.e., we would allow for duplicates to appear during insertion). Instead, once the delete operation is called on a key, we would calculate the index pertaining to the inserting key\u2014using the key's hash function\u2014and then <i>linearly scan</i> the <i>entire</i> linked list at the key's index to remove <i>all</i> instances of the key. This would result in a worst-case <i>linear</i> time complexity.<br></li></ul></li></ol><p>So why would we even choose to check for duplicates inside the <b>delete</b> operation to begin with? Notice that by doing so, we eliminate the need to check for duplicates inside the <b>insert</b> operation. Consequently, we are able to avoid having to linearly scan inside the <b>insert</b> method and therefore only have to do a single operation to insert a key at the front of a linked list. The <b>insert</b> operation now becomes a worst-case constant time complexity operation! Note that our <b>delete </b>operation originally (without having to check for duplicates) had a worst-case <i>linear</i> time complexity because worst case scenario, we would have had to traverse an entire linked list to find the element to delete. By checking for duplicates during <b>delete</b>, our worst-case time complexity actually still stays the same.<br></p>", "video": null, "animation": null, "options": {}, "subtitle_files": [], "source": null, "subtitles": {}, "tests_archive": null, "feedback_correct": "", "feedback_wrong": ""}, "id": "121807", "time": "2017-11-07T22:24:00.366776"}