{"block": {"tests_archive": null, "feedback_correct": "", "subtitle_files": [], "text": "\n\n<p>When we used the unequal-length mapping (<b><span class=\"wysiwyg-color-green\">0</span></b>, <b><span class=\"wysiwyg-color-blue\">10</span></b>, <b><span class=\"wysiwyg-color-purple\">110</span></b>, <b><span class=\"wysiwyg-color-red\">111</span></b>), even though some (less frequent) symbols were encoded with more than log\u2082(<i>n</i>)\u2309 bits, because the <b></b><b>more frequent</b>\u00a0symbols were encoded with <b>less than \u2308log\u2082(</b><i><b>n</b></i><b>)\u2309 bits</b><b></b>, we achieved significantly better compression. How do I know what the \"best possible case\" is for compressing a given message? In general, the entropy (in \"bits\") of a message is\u00a0$ \\sum_{i} p_i \\log \\frac{1}{p_i}$over all possible symbols <i>i</i>, where <i>p\u1d62</i>\u00a0is the frequency of symbol <i>i</i>. This entropy is the <b>absolute lower-bound</b>\u00a0of how many bits we can use to store the message in memory. Below is the tree representing the improved symbol mapping, which happens to be the encoding tree that the Huffman algorithm would produce from our message:</p><p><span class=\"image-wrapper\"><img src=\"https://ucarecdn.com/6978e1ee-927f-46b9-9e52-7ee75d80ce9e/\" alt=\"\"></span></p><p>Huffman's algorithm can be broken down into three stages: <b><span class=\"wysiwyg-color-green\">Tree Construction</span></b>, <b><span class=\"wysiwyg-color-blue\">Message Encoding</span></b>, and <b><span class=\"wysiwyg-color-purple\">Message Decoding</span></b>. </p>\n\n", "feedback_wrong": "", "options": {}, "subtitles": {}, "name": "text", "source": null, "video": null, "animation": null}, "time": "2016-09-11T01:12:41.378130", "id": "96571"}