{"block": {"animation": null, "source": null, "text": "<p>To be able to understand the theoretical components behind\u00a0<b>information compression</b>, we must first dive into a key topic of information theory:\u00a0<b><span class=\"wysiwyg-color-red\">entropy</span></b>.</p><p>Student A:\u00a0<i>\"Isn't 'entropy' related to 'chaos theory'? It seems irrelevant to information compression.\"</i></p><p><i></i>Student B:\u00a0<i>\"I vaguely recall that word from Chemistry, but I only took the class to satisfy a major requirement so I wasn't paying attention.\"</i><br></p><p>Student C:\u00a0<i>\"I took biology courses for my General Science requirement because Bioinformatics is clearly the most interesting data science, so if this topic was only covered in Physics and Chemistry, it's clearly useless to me.\"</i></p><p><i></i>Although Student C started off so strongly, unfortunately, all three of these students are wrong, and entropy is actually very important to Computer Science, Mathematics, and numerous other fields. For our purposes, we will focus on the relationship between\u00a0<b><span class=\"wysiwyg-color-red\">entropy</span></b>,\u00a0<b><span class=\"wysiwyg-color-green\">data</span></b>, and\u00a0<b><span class=\"wysiwyg-color-blue\">information</span></b>.</p>", "feedback_correct": "", "feedback_wrong": "", "video": null, "name": "text", "subtitles": {}, "subtitle_files": [], "options": {}, "tests_archive": null}, "id": "96252", "time": "2016-09-20T01:14:59.156238"}