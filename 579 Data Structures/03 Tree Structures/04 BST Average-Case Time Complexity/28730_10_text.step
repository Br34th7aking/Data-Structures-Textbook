{"id": "110137", "block": {"name": "text", "tests_archive": null, "animation": null, "feedback_wrong": "", "subtitle_files": [], "video": null, "options": {}, "text": "<p>As can be seen, the average-case time complexity of a \"find\" operation in a <b><span class=\"wysiwyg-color-green\">Binary Search Tree</span></b> with <i>n</i> elements\u00a0is <b>O(log </b><i><b>n</b></i><b>)</b>. However, keep in mind that this proof depended on two key assumptions:</p><p></p><ol><li>All <i>n</i> elements are equally likely to be searched for<br></li><li>All <i>n</i>! possible insertion orders are equally likely<br></li></ol><p>It turns out that, unfortunately for us, real-life data do not usually follow these two assumptions, and as a result, in practice, a regular <b><span class=\"wysiwyg-color-green\">Binary Search Tree</span></b> does not fare very well. However, is there a way we can be a bit more creative with our binary search tree to improve its average-case (or hopefully even worst-case) time complexity? In the next sections of this text, we will discuss three \"self-balancing\" tree structures that come about from clever modifications to the typical <b><span class=\"wysiwyg-color-green\">Binary Search Tree</span></b> that will improve the performance we experience in practice: the\u00a0<b><span class=\"wysiwyg-color-blue\">Randomized Search Tree (RST)</span></b>,\u00a0<b><span class=\"wysiwyg-color-purple\">AVL Tree</span></b>, and\u00a0<b><span class=\"wysiwyg-color-red\">Red-Black Tree</span></b>.</p><p></p>", "source": null, "subtitles": {}, "feedback_correct": ""}, "time": "2016-09-12T22:28:25.207728"}