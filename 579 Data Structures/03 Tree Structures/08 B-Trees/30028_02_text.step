{"id": "122783", "block": {"name": "text", "tests_archive": null, "animation": null, "feedback_wrong": "", "subtitle_files": [], "video": null, "options": {}, "text": "<p>As it turns out, the CPU determines which data to put in which sections of memory based on <i>spatial</i> and <i>temporal</i> locality. In other words, with respect to spatial locality, if we are accessing data that is close to other data, the CPU will also load the close data into an L1 cache because it predicts we might need to soon access the close data. For example, take a look at the for-loop below:</p><p></p><pre><code class=\"cpp\">for(int i = 0; i &lt; 10; i++) {\n\n    a[i] = 0; //Along with a[0] initially, CPU will load a[1], a[2]... into close memory in advance\n\n}</code></pre><p></p><p>The CPU is going to try to load the entire array a into close memory (the L1 cache) because it predicts that we will probably need to access other sections of the array soon after (and what a coincidence, it's right)! <br></p><p>So, by knowing how the CPU determines which data is placed where in memory, what can this further tell us about accessing memory in tree structures? Well, it tells us that if a node holds multiple pieces of data (e.g. an employee object that contains a string name, int age, etc) , then by loading one piece of data from the node (the employee's name), the CPU will automatically load the rest of the data (the employee's age, etc) from the same node. This implies that accessing other data that is <i>inside</i> the node structure is expected to be faster than simply traversing to another node.</p>", "source": null, "subtitles": {}, "feedback_correct": ""}, "time": "2016-09-12T22:29:06.600005"}