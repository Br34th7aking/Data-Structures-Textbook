{"id": "122782", "time": "2016-09-09T16:58:30.054039", "block": {"subtitle_files": [], "text": "<p>As we've been introducing different tree structures and analyzing their implementations, we've actually been making an assumption that you probably have overlooked as being  not too important: all node accesses in memory take the same amount of time. In other words, as we traverse an arbitrary tree structure and visit a node, we assume that the time it takes to \"visit\" a node is the same for every node. So in practice, is this assumption that we have been making all along accurate?</p><p>Well as it turns out, it can't <i>possibly</i> be accurate based of the way memory in our computers is built. As you might already know, memory in a computer is based on a hierarchical system as shown below:</p><p class=\"wysiwyg-text-align-right\"></p><img src=\"https://ucarecdn.com/70e5f385-bec6-49cd-9c0b-90a0d4adcbd9/\" alt=\"\" height=\"324\" width=\"652\"><br>The basic idea behind the hierarchy is that the top layers of memory --- colloquially referred to as the \"closest\" memory --- take the shortest time to access data from. Why? Partly because they are much smaller and therefore naturally take less time to traverse to find the data. <br><br>Going back to tree traversals, consequently, if there happens to be a pointer to a node that we need to traverse that points to memory in an L2 cache, then it will take longer to traverse that node then it would take to traverse a node that is located in an L1 cache (and since pointers can really point anywhere, this situation happens more often than you may think). So how does the CPU (the Central Processing Unit) generally determine which data is placed where in memory <b>and</b> how can we take advantage of that knowledge to speed up our tree traversal <i>\ufeffin practice</i>?<br><br><b><span class=\"wysiwyg-color-red\">Note:</span></b> It is a common question to ask: why don't we just (somehow) fit our <i>entire</i> tree structure into a fast section of memory ---such as an L1 cache--- to ensure that we have fast constant accesses across the entire tree. The problem with doing so is that the sections of memory that are fast (such as the caches) are actually quite small in size. Consequently, as our data structures grow in size, they will not be able to fit in that small section of memory. You might also be thinking then: well why don't we just make that fast section of memory bigger? Well the bigger we make the memory, the slower it will be, and we don't want slower memory! <br>", "feedback_correct": "", "tests_archive": null, "subtitles": {}, "source": null, "name": "text", "video": null, "feedback_wrong": "", "options": {}, "animation": null}}