{"block": {"tests_archive": null, "feedback_correct": "", "subtitle_files": [], "text": "<p>We prefaced this discussion about probability with the intention of selecting a capacity for our <b><span class=\"wysiwyg-color-purple\">Hash Table</span></b> that would help us <i>avoid</i> collisions in the average case. Mathematically, this translates into the following question: \"What should be the capacity of our <b><span class=\"wysiwyg-color-purple\">Hash Table</span></b>\u00a0in order to keep the <i>expected</i> (i.e., average-case) number of collisions relatively small?\" Let's see how we can go about computing the <b>expected total number of collisions</b> for <i>any</i> arbitrary\u00a0<b><span class=\"wysiwyg-color-purple\">Hash Table</span></b>. <br></p><p>Suppose we are throwing <i>N</i> cookies into <i>M</i> glasses of milk. The first cookie lands in some glass of milk. The remaining <i>N</i>\u20131 cookies have a probability $ \\frac{1}{M} $\u00a0of landing in the <i>same</i> glass of milk as the first cookie, so the average number of collisions with the <i>first</i> cookie will be $ \\frac{N-1}{M} $\u00a0(i.e., $ \\frac{1}{M} $\u00a0added\u00a0 <i>N</i>\u20131\u00a0times to account for the same probability for each remaining cookie left to throw). <br></p><p>Now, let's say that the second cookie lands in some glass of milk. The remaining <i>N\u2013</i>2 cookies each have probability $ \\frac{1}{M} $\u00a0of landing in the same glass of milk as the second cookie, so the number of collisions with the second cookie will be $ \\frac{N-2}{M} $, etc.\u00a0</p><p class=\"wysiwyg-text-align-center\"> So, the <b>expected total number of collisions</b> is $\\sum_{i=1}^{N-1} \\frac{i}{M} = \\frac{N(N-1)}{2M}$\u00a0</p><p>If we want there to be 1 collision on average (which translates into the O(1) average-case time complexity we desire), we can see that the expected number of collisions will be 1 when $\\frac{N(N-1)}{2M} = 1$, which, for a large\u00a0<i>M</i>, implies $N = \\sqrt{2M}$. In other words, if we will be inserting\u00a0<i>N</i> elements into our Hash Table, in order to keep the make the expected number of collisions equal to 1, we need our <b><span class=\"wysiwyg-color-purple\">Hash Table</span></b> to have a capacity\u00a0$ M = \\textrm{O}(N^2)$.</p><p>We hope that the underlying logic is somewhat intuitive:the more\n extra space you have, the lower the expected number of collisions. As a result, we also expect a better average-case performance.  However, as you might have inferred, this is extremely wasteful space-wise! In practice, making the expected number of collisions exactly 1 is a bit overkill. It turns out that, by some proof that is a bit out-of-scope for this text, we can formulate the expected number of operations a <b><span class=\"wysiwyg-color-purple\">Hash Table</span></b> will perform as a function of its <b><span class=\"wysiwyg-color-green\">load factor</span></b>\u00a0$ \\alpha = \\frac{N}{M} $:</p><p class=\"wysiwyg-text-align-center\">$ E(number\\ of\\ operations) = \\frac{1}{2} (1 + \\frac{1}{1 - \\alpha}) $\u00a0<br></p><p>If we were to plot this function, we would get the following curve:</p><p><span class=\"image-wrapper\"><img src=\"https://ucarecdn.com/712649f2-0b3c-40e6-8308-4ddafc27171a/\" alt=\"\" title=\"Image: https://ucarecdn.com/712649f2-0b3c-40e6-8308-4ddafc27171a/\"></span></p><p>Notice that the expected number of operations stays pretty flat for quite some time, but when it reaches a load factor somewhere around 0.75, it begins to jump up drastically. By general \"rule of thumb,\" it has been shown that we can experience O(1) performance on average when\u00a0$ \\alpha = \\frac{N}{M} \\approx 0.75 \\rightarrow M \\approx 1.3N $. As a result, if we have a prior estimate of the magnitude of\u00a0<i>N</i>, the number of elements we wish to insert, we would want to allocate a backing array of size approximately 1.3<i>N</i>. Also, throughout the lifetime of a specific instance of our\u00a0<b><span class=\"wysiwyg-color-purple\">Hash Table</span></b>, if we ever see that the\u00a0<b><span class=\"wysiwyg-color-green\">load factor</span></b> is approaching 0.75, we should consider resizing our array (typically to twice the prior capacity) to keep it low.</p><p>With regard to resizing the backing array, however, you should note that, because one step of our indexing process was to mod by\u00a0<i>M</i>\u00a0in order to guarantee valid indices, we would need to re-hash (i.e., reinsert) all of the elements from the old array into the new one from scratch, which is a O(<i>n</i>) operation. In order to avoid performing this slow resizing operation too often, we want to make sure to allocate a reasonably large array right off the bat.</p>", "feedback_wrong": "", "options": {}, "subtitles": {}, "name": "text", "source": null, "video": null, "animation": null}, "time": "2016-09-11T01:10:36.499728", "id": "121800"}