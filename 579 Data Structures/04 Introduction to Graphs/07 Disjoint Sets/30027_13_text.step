{"block": {"animation": null, "source": null, "text": "An Up-Tree with an implementation of <b><span class=\"wysiwyg-color-red\">path compression</span></b> is an example of a\u00a0<span class=\"wysiwyg-color-green\"><b><span class=\"wysiwyg-color-black\">self-adjusting structure</span></b></span>. <span class=\"wysiwyg-color-black\">In a self-adjusting structure, an operation like the \"find\" operation occasionally incurs a high cost because it does extra work to modify the structure of the data (such as the reattachment of vertices directly to the root in the example of <b><span class=\"wysiwyg-color-red\">path compression</span></b>\u00a0we saw previously). However, the hope is that subsequent similar operations will be made <i>much </i>more efficient. </span><span class=\"wysiwyg-color-green\"><span class=\"wysiwyg-color-black\">Examples of other self-adjusting structures are splay trees, self-adjusting lists, skew heaps, etc. (just in case you're curious to see what else is out there).</span></span><br><br><p><span class=\"wysiwyg-color-green\"><span class=\"wysiwyg-color-black\">So how do we figure out if the strategy of self-adjustment is actually worth it in the long run? The answer lies in </span></span><span class=\"wysiwyg-color-green\"><b><span class=\"wysiwyg-color-black\">amortized cost analysis</span></b></span><span class=\"wysiwyg-color-green\"><span class=\"wysiwyg-color-black\">.</span></span></p><p><span class=\"wysiwyg-color-green\"><span class=\"wysiwyg-color-black\">In regular algorithmic analysis, we usually look at the time or space cost of doing a single operation in either the best case, average case, or worst case. However, if we were to do so for an operation that involved something like <b><span class=\"wysiwyg-color-red\">path compression</span></b>, we would not get the tightest complexity because we already know that, based on how <b><span class=\"wysiwyg-color-red\">path compression</span></b> was designed, the first time we use it will guaranteed take longer than subsequent times. </span></span><span class=\"wysiwyg-color-green\"><b><span class=\"wysiwyg-color-black\">Amortized cost analysis</span></b></span><span class=\"wysiwyg-color-black\"> considers the time or space cost of doing a <i>sequence of operations</i> (as opposed to a single operation)\u00a0because the total cost of the entire sequence of operations might be less with the intial extra initial work than without!</span><span class=\"wysiwyg-color-green\"><span class=\"wysiwyg-color-black\"><br></span></span></p><p>We will not go into the details of how to actually perform amortized cost analysis because the math can get quite intense. Nonetheless, a mathematical function that is common in amortized cost analysis is log*(<i>N</i>) (read: \"log star of <i>N</i>\" and also known as the \"single variable inverse Ackerman function\"), which is equal to the number of times you can take the log base-2 of <i>N</i>, until you get a number less than or equal to 1. For example,<br></p><ul><li>log*2 = log* 2^1 = 1</li><li>log*4 = log * 2^2 = 2 (note that 4 = 2^2)<br></li><li>log*16 = log * 2^4 = 3 (note that 16 = 2^(2^2))</li><li>log*65536 = log*2^16 = 4 (note that 65536 = 2^(2^(2^2)))</li><li>log*2^65536\u00a0 = 5 (note that 2^65536 = 2^(2^(2^(2^2))) is a huge number)<br></li></ul><p>The reason we use this particular function is because it is able to take into consideration both aspects of a self-adjusting operation: <br></p><ul><li>The first non-constant time part (log base-2 in this case) in which the structure is re-adjusting itself</li><li>The second constant time part (less than or equal to 1 in this case) in which the structure has finished re-adjusting and is now reaping the benefits of the self-adjustment<br></li></ul><p>It can be shown that, with <b><span class=\"wysiwyg-color-blue\">Union-by-Size</span></b> or <b><span class=\"wysiwyg-color-purple\">Union-by-Height</span></b>, using <b><span class=\"wysiwyg-color-red\">path compression</span></b>\u00a0when implementing \"find\" performs any combination of up to <i>N</i>-1 \"union\" operations and <i>M</i>\u00a0\"find\" operations and therefore yields a worst-case time complexity of <b>O(<i>N</i> + <i>M</i> log* <i>N</i>)</b> <i>over all operations</i>. This is much better than old-fashioned logarithmic time because log*<i>N</i> grows <i>much </i>slower than log <i>N</i>,\u00a0and for all practical purposes, log*<i>N</i> is never more than 5 (so it can be considered practically constant time for reasonable values of\u00a0<i>N</i>). Therefore, <i>for each individual find or union operation</i>, the worst-case time complexity becomes constant (i.e., <b>O(1)</b>). <br></p>", "feedback_correct": "", "feedback_wrong": "", "video": null, "name": "text", "subtitles": {}, "subtitle_files": [], "options": {}, "tests_archive": null}, "id": "117405", "time": "2016-09-20T01:12:20.865822"}